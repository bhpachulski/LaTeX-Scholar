% Ver trabalho do slide 12 da Tati da apresentacao feita pra Solange e Heloisa.

\chapter{Extração de Termos para a Mineração de Textos}
\label{extracaoTermos}
%   3.1 Considerações Iniciais
%		3.2 Termos Simples e Compostos (tirei)
%   3.3 Identificação dos Termos
%		3.4 Técnicas de simplificação de termos
%       3.4.1 Radicalização
%       3.4.2 Lematização
%				3.4.3 Substantivação
%   3.5 Extração de Termos Simples e Compostos
%		3.6 Vocabulário Controlado
%		3.7 Trabalhos Relacionados com a Extração de Termos
%   3.7 Ferramentas para Extração de termos (onde??)
%   3.8 Considerações Finais


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Considerações Iniciais
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Considerações Iniciais}
O resultado da etapa do \textbf{Pré-Processamento}, no processo de Mineração de Textos, é uma re\-pre\-sen\-ta\-ção da coleção de textos a ser analisada em um formato adequado à etapa de Extração de Padrões. A qualidade dos resultados depende principalmente do trabalho realizado nesta etapa, mais especificamente nas tarefas de seleção e extração dos termos.

%% DÚVIDA: como irei tratar qdo o doc que apresenta caracteres inválidos ou palavras com erros gramaticais?? Salton (1983) diz pra usar Dictionary lookup.

O \textbf{termo}, também chamado de \textbf{característica} ou \textbf{atributo}, pode ser uma palavra simples ou composta. Quando o termo é composto por apenas uma palavra, este é denominado de \textit{unigrama} ou termo simples, e quando é composto por mais de uma palavra, é chamado de \textit{n-grama} (termo composto ou combinação). Como exemplos de termos simples, pode-se citar: \textit{inteligencia}, \textit{artificial}, \textit{processo}; já como exemplos de termos compostos, têm-se: \textit{inteligencia\_artificial} e \textit{processo\_mineracao\_textos}. Deve-se ressaltar que o significado semântico desses termos pode diferenciar quando são compostos de uma única palavra, como \textit{inteligencia} e quando são compostos de mais de uma palavra, como \textit{inteligencia artificial}.

Na etapa de Pré-Processamento, deve-se extrair termos (simples e/ou compostos) que conceitualmente melhor representam tais coleções. Isso ajudará a reduzir o número de termos utilizados, restringindo-os a um conjunto mais representativo da coleção, a fim de que o processamento desses dados seja uma tarefa computacionalmente mais simples e semanticamente adequada ao domínio de conhecimento.

Ressalta-se que alguns pesquisadores da área consideram a extração de termos como sinônimo para geração de termos, porém, neste trabalho, estas palavras são consideradas como distintas. A geração de termos é o processo de obter um conjunto de palavras com significado importante para a coleção de textos de um determinado domínio, sendo que estas palavras não são modificadas e, sim somente obtidas da coleção. Já a extração de termos, que é o caso deste trabalho, está relacionado à criação de um novo conjunto de termos que possua significado importante para a coleção de textos de um determinado domínio, como termos radicalizados, lematizados ou substantivados.

A etapa de Pré-Processamento, na qual a atividade de extração de termos ocorre, exige um cuidadoso planejamento e acompanhamento. Esse processo é interativo e bastante trabalhoso, e pode ter alto custo computacional.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Técnicas de Simplificação de Termos
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Técnicas de Simplificação de Termos}
\label{tecnicasSimplif}

A extração de termos, que visa reconhecer os candidatos a termos em uma coleção de textos, pode ser auxiliada por meio de buscas por padrões que simplifiquem as diversas formas de apresentação de termos com o mesmo significado essencial ou termos que utilizados em conjunto mo\-di\-fi\-quem o significado dos mesmos isoladamente. Entre os padrões ou simplificações mais utilizados encontram-se as técnicas de radicalização, lematização e substantivação, além da possibilidade de utilizar termos compostos e vocabulário controlado, como explicado a seguir.

%----------------------------------------------------------------
%   Radicalização = Stemmização
%----------------------------------------------------------------
\subsection{Radicalização} %arrumar: falar na pág 347 da Solange

A radicalização, também conhecida como ``Stemmização'' ou \textit{Stemming}, é uma técnica antiga muito utilizada. O primeiro trabalho encontrado na literatura sobre esta técnica é o de \citet{Lovins:1968}. A radicalização tem como objetivo reduzir as palavras às suas formas inflexionáveis e às vezes reduzir às suas derivações \citep{ManningRagSchutze:2008}. Para isto, a radicalização reduz cada palavra do texto ao seu provável radical, ou seja, palavra raiz (\textit{stem}), em que cada palavra é analisada isoladamente. Segundo \citet{Aranha:2007}, a radicalização pode ser vista como \textit{radicalização inflexional}, em que se considera apenas as remoções de flexões verbais, ou \textit{radicalização para a raiz}, na qual se realiza a remoção de todas as formas de prefixos e sufixos dos termos, sendo esta última a forma mais agressiva de radicalização. A seguir é mostrado um exemplo de radicalização para a raiz.

\begin{center}
Frase exemplo: Brasileiros pesquisam perfil do estudante.
\end{center}

Considerando a remoção de \textit{stopwords}, como resultado da radicalização para este exemplo tem-se:

\begin{center}
brasil pesquis perfil estudant
\end{center}

O processo de radicalização pode depender da linguagem, por normalmente necessitar de conhecimento lingüístico \citep{SillaKaestner:2002}. No entanto, deve-se atentar aos possíveis erros resultantes de análise incorreta do sentido das palavras, já que tais algoritmos ignoram o significado dos termos resultando possivelmente em alguns erros.

%arrumar: Preciso pegar o artigo do Porter (1980) e do Lovins (1968), pois só tenho as referências.
Os algoritmos de radicalização realizam a eliminação de prefixos e sufixos das palavras ou a transformação de um verbo para sua forma infinitiva. Porém, durante este processo, podem ocorrer dois tipos de erros: \textit{overstemming} e \textit{understemming}. O \textit{\textbf{overstemming}} acontece quando a parte removida da palavra não é um sufixo, e sim parte do seu radical. Este erro pode acarretar na possibilidade da combinação de palavras não relacionadas. Já o erro de \textit{\textbf{understemming}} acontece quando não se remove completamente um sufixo da palavra. Ao contrário do \textit{overstemming}, quando ocorre \textit{understemming} pode-se fazer com que não haja a combinação de palavras relacionadas. Por exemplo, o \textit{stem} correto da palavra \textit{inteligencia} é \textit{intelig}, mas quando ocorre o erro de \textit{overstemming}, o resultado da aplicação da técnica de radicalização pode ser \textit{intel}; e quando ocorre o erro de \textit{understemming} o resultado pode ser \textit{inteligenc}.

Como mostrado na Tabela \ref{tab:algRadic}, existem vários algoritmos de radicalização destinados a diferentes línguas. Dentre os mais conhecidos na literatura, podem-se citar o Método de Lovins \citep{Lovins:1968}, o Método de Porter (\textit{Porter Stemming Algorithm}) \citep{Porter:1980} e o Método \textit{Stemmer} S \citep{Harman:1991}. Sendo estes métodos desenvolvidos para a Língua Inglesa.



\begin{table}[!ht]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\raisebox{0.0ex}[0pt][0pt]{\bf \it \textbf{Língua}} & {\bf \it \textbf{Algoritmo}} & {\bf \it \textbf{Autoria}}\\
\hline \hline
\multirow{7}{*}{Inglês} &	Dawson & Dawson\\
& Stemmer S &	Harman\\
& Lovins &	Lovins\\
& KStem &	Krovetz\\
& Paice/Husk &	Paice e Husk\\
& Porter &	Porter\\
& Porter 2 &	Porter\\
\hline
\multirow{3}{*}{Português} & STEMBR & Alvares\\
& Pegastemming & Gonzalez\\
& PortugueseStemmer &	Orengo\\
&	Porter - Português & Porter\\
\hline
\multirow{2}{*}{Alemão} &	Porter - Alemão &	Porter\\
& Porter - Alemão - Variação &	Porter\\
\hline
Amárico (etíope) & Alemayehu-Willett &	Alemayehu e Willett\\
\hline
Búlgaro &	BulStem &	Nakov\\
\hline
Dinamarquês &	Porter - Dinamarquês &	Porter\\
\hline
Esloveno &	Popovic-Willett &	Popovic e Willett\\
\hline
\multirow{2}{*}{Espanhol} & Honrado et al. & Honrado et al.\\
&	Porter - Espanhol &	Porter\\
\hline
Finlandês &	Porter - Finlandês &	Porter\\
\hline
Francês &	Porter - Francês &	Porter\\
\hline
Galego &	Galician stemmer &	Brisaboa\\
\hline
\multirow{2}{*}{Holandês} & Kraaij-Pohlmann &	Kraaij e Pohlmann\\
&	Porter - Holandês &	Porter\\
\hline
Italiano &	Porter - Italiano &	Porter\\
\hline
Latim &	Schinke et al. &	Schinke et al.\\
\hline
\multirow{2}{*}{Norueguês} & Carlberger et al. &	Carlberger et al.\\
&	Porter - Norueguês &	Porter\\
\hline
Russo &	Porter - Russo &	Porter\\
\hline
Sueco &	Porter - Sueco &	Porter\\
\hline
Turco &	Ekmekçioglu et al. &	Ekmekçioglu et al.\\
\hline
\end{tabular}
\caption{Algoritmos para radicalização - Adaptada de \citet{VieraVirgil:2007}} \label{tab:algRadic}
\end{table}


%arumar: pesquisar algo mais sobre estes métodos
O \textbf{método de Lovins} é executado em um único passo, removendo no máximo um sufixo por palavra (o sufixo mais longo). Este método é considerado mais agressivo do que os métodos de Porter e \textit{Stemmer} S.

O \textbf{Método de Porter} foi originalmente proposto para a formação de radicais para a Língua Inglesa, isto é, geração dos radicais a partir da remoção dos sufixos das palavras. É considerado um algoritmo simples e muito eficiente para a radicalização de termos. Enquanto o Método de Lovins é executado em um único passo, este método é executado em cinco passos, sendo que cada passo realiza uma transformação sobre o termo alvo. Cada passo é formado por um conjunto de regras do tipo: \textit{se um termo $\textbf{t}$ possui mais do que $\textbf{s}$ sílabas e termina com o sufixo \textbf{SUFIX}, o sufixo \textbf{SUFIX} é substituído por \textbf{SUF}}. Ao final dessas substituições, espera-se obter o radical do termo. %arrumar: ver qual parte deste parágrafo é de qual ref: PUC Rio Certificação Digital 0210681/CA e livro da Solange p. 348, 2003)

\textbf{\textit{Stemmer} S} é considerado um método simples, conservador e raramente surpreende o usuário, pois somente remove alguns finais de palavras, como \textit{ies}, \textit{es} e \textit{s}.

Já para a Língua Portuguesa, pode-se citar os algoritmos: Porter - Português, \textit{PortugueseStemmer}, \textit{Pegastemming} e STEMBR.

\textbf{Porter - Português} foi desenvolvido na linguagem de programação \textit{Snowball}\footnote{\textit{Snowball} - http://snowball.tartarus.org/index.php} em 2005, pelo mesmo autor do algoritmo de Porter para a Língua Inglesa, sendo baseado em regras de remoção de sufixos.

\textbf{\textit{PortugueseStemmer}}, desenvolvido por Viviane Orengo e Christian Huyck \citep{Orengo:2001}, mesmo não sendo baseado no algoritmo de Porter, utiliza regras para a remoção de sufixos. Além disso, o \textit{PortugueseStemmer} trata palavras exceções por meio do uso de um dicionário de 32 (trinta e dois) mil termos.

O \textbf{\textit{Pegastemming}}\footnote{\textit{Pegastemming} - \url{http://www.inf.pucrs.br/~gonzalez/ri/pesqdiss/analise.htm}}, desenvolvido por Gonzalez, realiza a remoção simples de sufixos comuns, sem se preocupar com artigos, preposições e conjunções.

O \textbf{STEMBR} \citep{AlvaresEtAl:2005}, mesmo não sendo baseado no método de Porter, também trabalha com conjunto de regras para a extração do \textit{stem}. O STEMBR remove os prefixos e sufixos das palavras por meio do tratamento baseado em estudo estatístico das freqüências das palavras contidas em páginas Web até o ano de 2005.

Como exemplos de aplicações dos algoritmos descritos, pode-se citar o Stemmer \citep{CaldImaRezende:2001}, PreTexT \citep{Matsubara:2003} e Lucene \citep{Lucene:2005}.

A ferramenta \textbf{Stemmer} \citep{CaldImaRezende:2001} foi desenvolvida no LABIC\footnote{LABIC - http://labic.icmc.usp.br/} (Laboratório de Inteligência Computacional do ICMC/USP) baseada no algoritmo de Porter e extrai \textit{stems} de palavras do português do Brasil, para isso a ferramenta remove os sufixos e terminações destas palavras.

A ferramenta \textbf{PreTexT}, desenvolvida no LABIC inicialmente por \citet{Matsubara:2003} e posteriormente atualizada por \citet{pretext2:2008} (PreTexT II), tem como objetivo au\-xi\-li\-ar na etapa de Pré-Processamento de uma coleção de documentos, apresentando facilidades para reduzir a dimensionalidade do conjunto de termos. Para isso, possui uma implementação do algoritmo do Porter utilizando o paradigma de orientação a objetos em Perl. Tal implementação possibilita extrair \textit{stems} de palavras nas Línguas Portuguesa, Espanhola e Inglesa. O algoritmo da PreTexT verifica se os sufixos da palavra possuem comprimento mínimo estabelecido, considerando algumas regras pré-estabelecidas. Caso possuem, estes sufixos são eliminados da palavra. Porém, devido às línguas provenientes do latim terem formas verbais conjugadas em sete tempos, cada uma com seis terminações diferentes, foi necessário um tratamento para estas terminações. Então, para as Línguas Portuguesa e Espanhola, caso não seja possível eliminar, de acordo com essas regras, nenhum desses sufixos, as terminações verbais da palavra são analisadas. A ferramenta disponibiliza também uma lista de \textit{stopwords} que pode ser incrementada manualmente pelo usuário. Quanto ao uso de termos, a PreTexT possibilita gerar os termos simples (\textit{unigrama}) ou compostos (mais de \textit{unigrama}) e, tem como saída vários arquivos com informações úteis para o usuário, como freqüência dos \textit{stems}, o quanto cada documento é esparso, freqüência das palavras que originam os \textit{stems} e outros. Além disso, permite, também, o uso de métodos de seleção de termos, como os cortes de Luhn \citep{Luhn:1958}. %, \textit{Term Frequency-Inverse Document Frequency - tf-idf} \citep{SaltonBuc:1987}, \textit{Term Frequency - tf}, \textit{Term Frequency Linear - tf-linear} \citep{Matsubara:2003}.
Para aplicá-los, a PreTexT oferece uma opção de utilizar somente os \textit{stems} que estão em um determinado intervalo de freqüência ou usar os pontos de corte superior e inferior que são encontrados empiricamente pelo usuário \citep{MartMatsubMon:2003}.

%tirado no artigo que eu revisei nº53171 do Semish, no qual cita referência do Lucene como sendo: Gospodnetic, O.; Hatcher, E. Lucene In Action: A guide to the Java search engine. Manning Publications Co. 2005.
O \textbf{Lucene} \citep{Lucene:2005} é uma API que contém classes desenvolvidas utilizando a linguagem de programação Java que executam atividades de Mineração de Textos. Dentre estas classes há duas específicas para realizar a radicalização em textos na Língua Portuguesa, a \textit{BrazilianStemFilter} e a \textit{BrazilianStemmer}, que são baseadas no algoritmo de Porter.

 
%----------------------------------------------------------------
%   Lematização
%----------------------------------------------------------------
\subsection{Lematização}

A técnica de lematização, também conhecida como Redução à Forma Canônica, tem como objetivo agrupar as variantes de um termo em um único lema, ou seja, transformar verbos para sua forma no infinitivo, e substantivos e adjetivos para o masculino singular. Pode-se observar um exemplo da redução das palavras ao seu lema na Tabela~\ref{tab:exe-Lema}, no qual são mostrados os lemas e exemplos de flexões das mesmas.

\begin{table}[!ht] \footnotesize \centering
\begin{tabular}{|c|c|c|c|} \hline
 $Lema$                      & $Singular Fem.$           & $Plural Fem.$         & $Plural Masc.$\\ \hline \hline
 $\texttt{brasileiro}$ & $\texttt{brasileira}$ & $\texttt{brasileiras}$ & $\texttt{brasileiros}$\\
 $\texttt{pesquisa}$   & $\texttt{pesquisa}$   & $\texttt{pesquisas}$   & $\texttt{pesquisas}$\\
 $\texttt{perfil}$     & $\texttt{perfil}$       & $\texttt{perfis}$        & $\texttt{perfis}$\\
 $\texttt{estudante}$  & $\texttt{estudante}$  & $\texttt{estudantes}$  & $\texttt{estudantes}$\\ \hline
\end{tabular}
\caption{Exemplos de lematização} \label{tab:exe-Lema}
\end{table}


Para a Língua Portuguesa, foram encontrados alguns etiquetadores morfossintáticos que podem auxiliar no processo de lematização. No processo de etiquetagem cada termo de um texto é associado à uma etiqueta (\textit{tag}), que corresponde a sua classe gramatical, como verbo, substantivo e adjetivo. Segundo \citet{HonoratoMonard:2008}, o processo de etiquetagem, normalmente, tem custo de tempo alto e está sujeito à erros. Os etiquetadores encontrados são: o etiquetador de BRILL \citep{Brill:1995} e o MXPOST \citep{Ratn:1996}.

O \textbf{etiquetador de BRILL} é um marcador morfossintático de palavras de um texto baseado em aprendizado computacional, ou seja, o aprendizado de uma série de regras contextuais que são utilizadas na etiquetagem.

O \textbf{MXPOST} (\textit{Maximum entropy pos tagger}) \citep{Ratn:1996} é um etiquetador morfossintático disponível na Web para uso não comercial e foi implementado, usando a linguagem de programação Java (JDK 1.1), por um grupo de pesquisadores da Universidade da Pensilvânia. Seu objetivo é fazer uma análise sintática, colocando em arquivos textos as marcações \textit{tag} que identificam a classificação gramatical da palavra dentro da frase. 

Após a identificação das classes gramaticais dos termos a partir do processo de etiquetagem, é possível, então, reduzir tais palavras ao seu lema. Existem ferramentas de lematização encontradas na literatura, que são descritas a seguir, como a TreeTagger \citep{Schmid:1994}, o Lematizador de Nunes \citep{Nunes:1996}, o FLANOM \citep{flanom:1999}, a FORMA \citep{GonzalezLima:2006} e o \textbf{Sphinx}\footnote{Sphinx - http://www.sphinxbrasil.com.br/}.

O \textbf{TreeTagger}\footnote{TreeTagger - http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/} \citep{Schmid:1994} foi desenvolvido por Helmut Schmid em 1994 para o Projeto TC do Instituto para Computação Lingüística da Universidade de Stuttgart. É uma ferramenta para etiquetagem morfossintática e um lematizador, podendo ser utilizado para as Línguas Alemã, Búlgara, Chinesa, Espanhola, Francesa, Grega, Holandesa, Inglesa, Italiana, Portuguesa e Russa.

O \textbf{Lematizador de Nunes} é uma ferramenta disponível gratuitamente desenvolvida por Nunes e seus colaboradores \citep{Nunes:1996} direcionado à Língua Portuguesa.

O \textbf{FLANOM} (\textit{Flexionador y lematizador automático de formas nominales}) é um lematizador de palavras na Língua Espanhola desenvolvido por \citet{flanom:1999}.

A ferramenta \textbf{FORMA}\footnote{FORMA - http://www.inf.pucrs.br/~gonzalez/tr+/forma/}, desenvolvida por \citet{GonzalezLima:2006}, também é direcionada à Língua Portuguesa. Essa ferramenta primeiramente \textit{toqueniza} as palavras do texto, em seguida, as etiqueta morfologicamente para, então, lematizá-las.

O software proprietário \textbf{Sphinx}, versão 4, possibilita a aplicação da técnica de lematização nos textos das Línguas Francesa e Inglesa.



%----------------------------------------------------------------
%   Substantivação
%----------------------------------------------------------------
\subsection{Substantivação}
É um processo, também conhecido por ``Nominalização'', na qual as palavras passam a exibir um comportamento sintático/semântico semelhante àquele próprio de um nome\footnote{Texto sobre Gramática Tradicional e Categorização Lexical - \\ http://www.dacex.ct.utfpr.edu.br/paulo3.htm}. Deve-se ressaltar que a maioria das palavras do português podem ser nominalizadas com o uso de artigos. A seguir, é mostrado um exemplo de substantivação.

\begin{center}
Frase exemplo: Técnicas relacionadas à Inteligência Artificial.
\end{center}

Considerando a remoção de \textit{stopwords} e limpeza do texto, tem-se como resultado da substantivação para este exemplo:

\begin{center}
tecnica relacionar inteligencia artificial
\end{center}

Para a Língua Portuguesa, pode-se citar a combinação das ferramentas CHAMA e FORMA, desenvolvidas por \cite{GonzalezLima:2006}. A ferramenta FORMA tem como objetivo \textit{toquenizar} e etiquetar morfologicamente as palavras dos textos, resultando em palavras lematizadas. Este resultado serve como entrada para a ferramenta CHAMA que é responsável pela nominalização de adjetivos, advérbios e verbos nos textos, ou seja, a transformação destas palavras em substantivos.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Extração de Termos Simples e Compostos
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extração de Termos Simples e Compostos}
%Exemplo: Brasileiro pesquisa perfil do estudante.

A extração de termos consiste em, a partir da extração de palavras de documentos de um domínio, obter um novo conjunto de termos que representam tal domínio a ser trabalhado. A extração de termos, de acordo com \citet{Teline:2003}, possui três abordagens principais, que são: estatística, lingüística e híbrida.

A abordagem \textbf{Estatística} utiliza somente métodos baseados em conhecimento estatístico e é utilizada sobre a forma de representação de termos \textit{bag of words}, nos quais os termos são tratados como um conjunto desordenado de palavras independentes. Assim, termos são considerados independentes entre si e todas as inferências são realizadas sobre algum valor dado a esses termos, como por exemplo, suas respectivas freqüências na coleção de textos.

A abordagem \textbf{Lingüística} utiliza métodos baseados em conhecimento lingüístico. Estes métodos podem fazer uso de recursos que contêm diferentes informações lingüísticas para a extração dos termos, como informações lexicográficas (dicionários de termos e \textit{stoplist}), informações morfológicas (padrões de estrutura interna das palavras), informações morfossintáticas (categorias morfossintáticas e funções sintáticas), informações semânticas (classificações semânticas) e informações pragmáticas (representações tipográficas e informações de disposição do termo no texto). E, por fim, a abordagem \textbf{Híbrida} faz uso das duas abordagens (estatística e lingüística).

Independente da abordagem escolhida para ser utilizada, a atividade de extração de termos é completada quando se tem somente os termos que representam a coleção de textos ou a maioria destes. Estes termos podem ser termos simples ou combinações de termos, ou seja, seqüências de duas ou mais palavras que possuem características sintáticas e semânticas de uma unidade.

O significado exato e desambíguo ou conotação destas palavras não pode ser diretamente derivado dos significados ou conotações de seus componentes. Deve-se, portanto, considerar o comportamento e sentido especial destas palavras consecutivas \citep {Choueka:1988}. Para melhor entendimento, pode-se observar os diferentes significados das combinações a seguir: \textit{inteligencia}, \textit{inteligencia emocional}, \textit{inteligencia artificial}, \textit{inteligencia policial}, \textit{inteligencia musical} e \textit{inteligencias multiplas}. Um método simples para encontrar essas combinações em um texto é a contagem de ocorrência das mesmas. Este método con\-si\-de\-ra que se duas palavras ocorrem juntas diversas vezes, então é evidente que elas tenham um significado especial juntas, que não é o mesmo quando separadas. Deve-se notar que esse processo é puramente estatístico, pois as combinações das palavras são encontradas em um processo estocástico de co-ocorrência.

Entretanto, deve-se assumir que somente selecionando os \textit{n-gramas} (seqüência de `n' \textit{tokens}) mais freqüentes, não levará a um resultado completamente satisfatório, pois pode haver, por exemplo, uma alta freqüência das palavras ``\textit{e o}'', que dependendo do objetivo, não significará nada. Por isso, no processo de limpeza dos textos, anteriormente citado, é interessante elaborar uma lista de \textit{stopwords}, para eliminar palavras com menos significado. Outra solução in\-te\-res\-san\-te é combinar um pouco de conhecimento lingüístico, que pode auxiliar a correta identificação das funções sintáticas de cada termo \citep {ManniSchut:2001}.

Ressalta-se também que, ao escolher a quantidade máxima de \textit{gramas} que comporá o termo, deve-se levar em consideração que tal quantidade é proporcional ao número de possibilidades de combinações entre termos simples (\textit{unigramas}) e, conseqüentemente, proporcional ao número de termos extraídos.

Para a obtenção de \textit{n-gramas}, pode-se utilizar a ferramenta PreTexT, anteriormente citada, ou o pacote NSP.
%arrumar: conferir a definição de token para este trabalho!!
O pacote NSP (\textit{Ngram Statistics Package}) \citep{Pedersen:2003} foi implementado na linguagem de programação Perl e tem sido apoiado pela \textit{National Science Foundation Faculty Early Career Development Program} (CAREER). Em versões anteriores (v0.1, v0.3, v0.4), este pacote era conhecido como \textit{\textbf{Bigram} Statistics Package} (BSP), mas com o aumento da capacidade de trabalhar com \textit{n-gramas} e não mais somente \textit{Bigramas}, seu nome foi alterado para \textit{N-gramas}. Este pacote é composto por um conjunto de programas que auxilia na análise de \textit{n-gramas} em arquivos textos, ou seja, permite a identificação de \textit{n-gramas} nesses arquivos.    

Esses \textit{n-gramas} correspondem aos candidatos a termos da coleção de textos. Como a quantidade de candidatos a termos é muito elevada, faz-se necessário utilizar algum método para escolher os que devem ser removidos. Há alguns testes estatísticos que podem ser utilizados para este fim. A idéia fundamental é testar a dependência entre as palavras consideradas como partes do \textit{n-grama}. Para isso considera-se a freqüência de ocorrência de cada grama e de todas as suas combinações. Para testar a hipótese de dependência o estimador mais utilizado é o qui-quadrado, no entanto, para dados bastante esparsos, o estimador mais adequado é o logaritmo da razão de verossimilhança \citep{ManniSchut:2001}.   

%\textcolor{violet}{O teste \textit{t} (de Student) é um exemplo, no qual considera que as palavras da coleção (a hipótese) possuem uma distribuição normal. Já o teste $\chi^2$ (chi-quadrado) de Pearson pode ser utilizado sem assumir a normalidade da distribuição da variável medida. Há também o teste da razão de máxima verossimilhança (\textit{likelihood ratio}) que é uma abordagem para testar hipóteses não assumindo normalidade da distribuição} \citep{ManniSchut:2001}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Vocabulário Controlado (Categorias)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Vocabulário Controlado}
\label{vocabControlado}
Uma possível técnica de redução do número de termos em uma coleção de textos é o reconhecimento de sinônimos ou termos hierarquicamente superiores ou inferiores aos termos analisados, bem como a substituição dos termos analisados por esses, ou melhor, o uso de taxonomias ou \textit{thesaurus}. Considera-se que dois termos são sinônimos se existir algum contexto em que ambos puderem ser substituíveis sem provocar alteração substancial do significado \citep{Cruse:1986}.

Uma taxonomia é uma coleção de vocabulários controlados e organizados hierarquicamente (\citet {KilgaYallop:2000} apud \citet {Moura:2006}), enquanto um \textit{thesaurus} pode ser definido como um vocabulário controlado que representa si\-nô\-ni\-mos, hierarquias e re\-la\-cio\-na\-men\-tos as\-so\-ciativos entre termos.

O \textit{thesaurus} utiliza listas pré-compiladas de termos importantes para um determinado contexto, em que cada termo da lista é representado pela ligação (seja por relações de equivalência, hierarquia e/ou associação) com vários outros. Dessa forma, o seu uso facilita aos usuários encontrar as informações que necessitam, mesmo que façam várias consultas com termos distintos (que estejam ligados), obtendo os mesmos resultados devido às relações dos termos. Além disso, o \textit{thesaurus} diminui a quantidade de termos-índices quando utilizado no processo de normalização (como ocorre na técnica de radicalização).

Pode-se fazer uma comparação do \textit{thesaurus} com a técnica de simplificação de termos observando o resultado final de ambos. O \textit{thesaurus} resume várias palavras em apenas um termo, substituindo essas palavras por termos ligados entre si (como sinônimos). E técnicas de simplificação de termos, como por exemplo a radicalização, também resumem várias palavras em apenas um termo no momento em que substitui estas palavras por seus radicais.
%falar das taxonomias
%Exemplo: Brasileiro pesquisa perfil do estudante.

Como exemplos de \textit{thesaurus}, pode-se citar o Thesagro, o \textit{Thesaurus} da Língua Portuguesa do Brasil e o Tep.

%O \textbf{Thesagro}\footnote{Thesagro - \\ http://www.agricultura.gov.br/portal/page?\_pageid=33,959135\&\_dad=portal\&\_schema=PORTAL} (\textit{Thesaurus} Nacional Agrícola) é um aperfeiçoamento do \textit{thesaurus} construído para indexação e recuperação da literatura agrícola brasileira e publicado em junho de 1979 pela BINAGRI (Biblioteca Nacional de Agricultura, órgão da Secretaria de Executiva do Ministério da Agricultura, Pecuária e Abastecimento). A construção do Thesagro seguiu as diretrizes da UNESCO (normas estabelecidas pela \textit{United Nations Information System}) e atualmente contém 9.351 termos.

O \textbf{Thesagro} (\textit{Thesaurus} Nacional Agrícola), publicado pela BINAGRI\footnote{BINAGRI - http://www.agricultura.gov.br/} (Biblioteca Nacional de Agricultura, órgão da Secretaria de Executiva do Ministério da Agricultura, Pecuária e Abastecimento), é o único \textit{thesaurus} brasileiro especializado em literatura agrícola, além disso, contém informações sobre as relações hierárquicas dos seus termos, que são explicadas a seguir. Para exemplificar essas relações, são utilizados alguns termos extraídos do Thesagro\footnote{Thesagro - \\ http://www.agricultura.gov.br/portal/page?\_pageid=33,959135\&\_dad=portal\&\_schema=PORTAL}. A \textbf{relação de associação} (\textit{related term} (\textit{RT}) - termo relacionado) é empregado para estabelecer associação entre um termo cujo significado se relaciona semanticamente com outro termo, mas sem nenhuma ligação hierárquica entre si. Como exemplo, pode-se citar o termo \textit{enologia} cujo termo relacionado é \textit{vinho}. A \textbf{relação de equivalência} (\textit{USE}) é utilizada para indicar o termo correto a ser utilizado. Por exemplo: para o termo \textit{abacaxizeiro} deve-se usar (USE) o termo \textit{abacaxi}. A \textbf{relação hierárquica genérica} (\textit{broader term} (\textit{BT}) - termo genérico) é empregado para indicar um termo mais amplo, mais abrangente. Para o termo \textit{inseticida}, por exemplo, o termo genérico correspondente é \textit{defensivo}. A \textbf{relação hierárquica específica} (\textit{narrower term} (\textit{NT}) - termo específico) é utilizado para indicar termos mais definidos. Para o termo \textit{intoxicação}, por exemplo, tem-se como termos específicos a \textit{intoxicação animal} e a \textit{intoxicação vegetal}.


O \textbf{\textit{Thesaurus} da Língua Portuguesa do Brasil}\footnote{\textit{Thesaurus} da Língua Portuguesa do Brasil - http://alcor.concordia.ca/~vjorge/Thesaurus/} foi construído manualmente no ano de 2000 por Valdir Jorge e disponibilizado livremente na Web.


O \textbf{TeP} (\textit{Thesaurus} Eletrônico para o Português do Brasil) foi desenvolvido por \citet{Dias-da-Silva:2000} e posteriormente detalhado no trabalho de \citet{Dias-da-silva:2003}. É considerado um \textit{thesaurus} eletrônico para o português do Brasil, sendo considerado um dicionário eletrônico de sinônimos e antônimos, composto por substantivos, adjetivos, verbos e advérbios. A base de dados lexicais do TeP \citep{MazieroEtAl:2008} foi desenvolvida para servir como ponto de partida da rede WordNet, sendo, portanto, feita segundo o modelo da rede WordNet para o português do Brasil (WordNet.Br).

Na obtenção de sinônimos e antônimos, quando um determinado verbete é buscado na base de dados lexicais do TeP, caso este esteja presente, são retornados os conjuntos de sinônimos e antônimos do mesmo. Por exemplo: para a entrada (verbete) \textit{recordar}, é retornado seu conjunto correspondente, que é \textit{\{lembrar, recordar\}}, sendo \textit{lembrar} considerado como seu sinônimo. Caso o verbete ainda não exista na base do TeP, o mesmo é inserido como entrada na base, possibilitando a geração automática de novos verbetes, incluindo os seus conjuntos de sinônimos e de antônimos, se houver.

A base de dados lexicais do TeP possui mais de 19 mil conjuntos, que indexam 44 mil entradas distribuídas em 17 mil substantivos, 15 mil adjetivos, 11 mil verbos e 1 mil advérbios. Essa base é utilizada na \textbf{WordNet.Br} (WordNet para o Português do Brasil) \citep{Dias-da-Silva:2004} o que possibilita substituir palavras sinônimas em diversas frases do texto. Por exemplo, quando deseja-se evocar em um texto o sentido de ``\textit{examinar cuidadosamente}'', a WordNet.Br procura no próprio texto frases que contenham este sentido, como, por exemplo, os verbos: \textit{fiscalizar}, \textit{patrulhar}, \textit{policiar} e r\textit{ondar}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Trabalhos Relacionados com a Extração de Termos
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Trabalhos Relacionados com a Extração de Termos}
Devido a importância de se extrair termos nas mais diversas línguas, várias ferramentas e algoritmos de extração de termos têm sido desenvolvidos. Como exemplo, pode-se citar o trabalho de \citet{Smad:1993} no qual foi desenvolvida uma ferramenta lexográfica (Xtract) para extrair colocações da Língua Inglesa e, posteriormente, estendida com o nome CXtract, para a Língua Chinesa no trabalho de \citet{Fung:1998}. Já \citet{DagChu:1994} desenvolveram uma ferramenta que tem como objetivo auxiliar os terminologistas na identificação e tradução de termos técnicos.

No trabalho de \citet{HonoratoMonard:2008} foi desenvolvido um ambiente para extrair terminologia de forma híbrida a partir de laudos médicos, denominado \textit{Term Pattern Discover} (TP-Discover). Tal ambiente, resumidamente, seleciona palavras e frases que aparecem com uma determinada freqüência e, para isso, a técnica de lematização foi aplicada utilizando o lematizador TreeTagger \citep{Schmid:1994}. Então, selecionam-se os termos com uma determinada propriedade sintática.

Já como algoritmos de extração de termos que combinam a abordagem híbrida (técnicas estatísticas e conhecimento lingüístico), pode-se citar o algoritmo proposto por \citet{EkWille:1998}. O trabalho de \citet{MayAnan:1999} também utilizou a abordagem híbrida, atribuindo peso aos candidatos a termo de acordo com sua classe gramatical; já no ano de 2000, para extração de termos, esses autores consideraram o termo candidado e o termo de contexto (termo que aparece dentro de uma janela de tamanho fixo), utilizando três tipos de informação de contexto: sintática (atribui pesos para as diferentes classes gramaticais a que o termo candidato pertence), terminológica (atribui um peso ao termo candidato baseando-se nos termos de contexto dele) e semântica (mede a similaridade entre o termo candidato e os termos de contexto) \citep{MayAna:2000}.

No trabalho de \citet{LuccaNunes:2002}, as diferenças teóricas existentes entre as técnicas de lematização e radicalização são ressaltadas. Os autores afirmam que a lematização existe puramente no contexto lexicográfico, pois esta representa os adjetivos e substantivos por seu masculino singular e os verbos por seus infinitivos. Já a radicalização não existe puramente no contexto lexicográfico, pois esta remove os sufixos do radical, segundo o algoritmo de Porter. Mesmo assim, eventualmente, estas duas técnicas podem gerar resultados graficamente semelhantes.

\citet{Chaves:2003} fez uma análise de precisão de dois algoritmos de radicalização de palavras pertencentes à Língua Portuguesa. Para tal análise, foram considerados os algoritmos \textit{Pegastemming} e \textit{PortugueseStemmer}, e 500 \textit{stems} de palavras diversas foram obtidas manualmente para, em seguida, aplicar o processo automático de radicalização nestas mesmas palavras utilizando, separadamente, cada um desses algoritmos. Considerando que cada algoritmo foi desenvolvido para aplicações específicas conforme a necessidade de seu respectivo autor, foram apresentados diversos resultados positivos e negativos de cada algoritmo. Como por exemplo, o algoritmo \textit{Pegastemming} apresentou uma melhor precisão quando processadas palavras da categoria substantivo, porém o \textit{PortugueseStemmer} obteve a melhor precisão quando processados advérbios.

No trabalho de \citet{KorEtAl:2004}, 5.000 artigos publicados em jornais na Língua Finlandesa foram agrupados por quatro métodos de agrupamento hierárquico aglomerativo e, durante este processo, as técnicas de radicalização e lematização foram aplicadas. Foram obtidos melhores resultados quando utilizada a técnica de lematização, por outro lado, com o uso da radicalização a similaridade entre os documentos aumentou devido a junção maior de diferentes palavras quando utilizada esta técnica, já que o número de palavras discriminantes aumentou.

\citet{Santos:2005} desenvolveu um lematizador somente para verbos a partir do Banco de Conjugações de Verbos da Língua Portuguesa em sua versão 1.1, que faz parte do software livre Conjugue\footnote{Conjugue - \url{http://www.ime.usp.br/~ueda/br.ispell/}}. Este lematizador foi aplicado aos verbos da base de textos a fim de uniformizar as regras aprendidas para as tarefas de classificação. Como resultado, a aplicação do lematizador nos verbos obteve uma redução do tempo de treinamento e aumentou a abrangência do conjunto de regras aprendidas, mas acarretou em uma contribuição pouco significativa em termos de eficácia no resultado da aplicação das regras aprendidas.

No trabalho de \citet{GonzalezLima:2006}, foram apresentados a técnica de substantivação e um novo lematizador, ambos voltados para a Língua Portuguesa e implementados pelos autores nas ferramentas FORMA e CHAMA. Estas técnicas foram comparadas à radicalização com foco na recuperação de informação, sendo que para a radicalização utilizou-se o algoritmo \textit{PortugueseStemmer} \citep{Orengo:2001}. Nesta comparação, o uso da técnica de substantivação obteve diferença significativa positiva em relação às técnicas de radicalização e lematização.


Pode-se citar também a OntoLP \citep{Ribeiro:2008} que é um \textit{plug-in} desenvolvido para auxiliar de forma semi-automática os engenheiros de ontologias de Língua Portuguesa, mostrando sugestões de termos, conceitos e de organização de hierarquias da ontologia, com base no conhecimento contido em base textual ou corpus de um domínio específico. Este \textit{plug-in} serve para ser utilizado no editor de ontologias Protégé \citep{GennariEtAl:2002}, que oferece suporte à construção de ontologias, seguindo as tecnologias da Web Semântica, como a construção de ontologias OWL \textit{Web Ontology Language}. 


%??
%1 definições sobre lematização: EM BUSCA DE UMA ANÁLISE LEXICOGRÁFICA: ESTUDOS DE TEXTOS DE CONTOS MACHADIANOS %(C:\Mel\Projeto\bases usadas\EM BUSCA DE UMA ANÁLISE LEXICOGRÁFICA.pdf

%2. sobre a expressão ``extração automática de palavras-chaves'': EXTRAÇÃO AUTOMÁTICA DE PALAVRAS-CHAVE NA LÍNGUA PORTUGUESA APLICADA A DISSERTAÇÕES E TESES DA ÁREA DAS ENGENHARIAS %(C:\Mel\Projeto\bases usadas\Dias,MariaAbadiaLacerda.pdf

%3.ver se tem mais trabalhos relacionados no site do NILC


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Considerações Finais
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Considerações Finais}
Durante a atividade de extração de termos a partir de coleções textuais, que foi abordada neste capítulo, pode-se utilizar combinações das técnicas de simplificação dos termos, a fim de melhorar a representação dos mesmos na coleção de textos, conseqüentemente melhorar o resultado final do objetivo proposto pelo usuário.

Deve-se ressaltar que a opção pelo uso de formas de simplificação depende das metas pré-estabelecidas e tem como benefício melhor representar a coleção em questão, bem como auxiliar na redução da dimensionalidade da forma de representação dos termos extraídos, que no caso é a matriz atributo-valor. Dessa forma, pode-se minimizar um dos maiores problemas que é o de trabalhar com uma enorme quantidade de termos. Além de melhorar relativamente a busca de uma palavra pelo usuário em uma coleção de textos, pois possibilita retornar como resultado, não mais somente uma forma desta palavra, devido suas variações (plurais, formas de gerúndio, sufixos), e sim uma combinação entre uma palavra da consulta e uma palavra do documento, aumentando, portanto, a gama de busca deste usuário.

Com os conceitos apresentados neste capítulo, nota-se a necessidade de se escolher adequadamente técnicas de simplificações dos termos para o domínio e/ou o uso de algum \textit{thesaurus} para serem utilizadas na atividade de extração de termos.%, confirmando, portanto, o objetivo principal deste trabalho.

%\textcolor{red}{Thiago: nao entendi: ``Trabalhos relacionados do NILC/ICMC?''}

%A extração de termos feita pelo OntoLP diferencia deste trabalho, pois tem seu foco na lingüística e não utiliza técnicas de simplificação de termos, enquanto este trabalho faz uso de tais técnicas além de utilizar métodos estatísticos para auxiliar na extração de termos.